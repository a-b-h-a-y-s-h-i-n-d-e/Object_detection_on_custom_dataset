{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!unzip /content/detecting_speed_bumps.v4-marked-speed-bumps.voc.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools --quiet\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "!git checkout v0.3.0\n",
    "\n",
    "!cp vision/references/detection/utils.py ./\n",
    "!cp vision/references/detection/transforms.py ./\n",
    "!cp vision/references/detection/coco_eval.py ./\n",
    "!cp vision/references/detection/engine.py ./\n",
    "!cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# xml library for parsing xml files\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# these are the helper libraries imported.\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "files_dir = '/content/train'\n",
    "test_dir = '/content/test'\n",
    "\n",
    "\n",
    "class SpeedBreakerDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, files_dir, width, height, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.files_dir = files_dir\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        # sorting the images for consistency\n",
    "        # To get images, the extension of the filename is checked to be jpg\n",
    "        self.imgs = [image for image in sorted(os.listdir(files_dir))\n",
    "                        if image[-4:]=='.jpg']\n",
    "\n",
    "\n",
    "        # classes: 0 index is reserved for background\n",
    "        self.classes = ['background', 'speed-bumps']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      img_name = self.imgs[idx]\n",
    "      image_path = os.path.join(self.files_dir, img_name)\n",
    "\n",
    "      # reading the images and converting them to correct size and color\n",
    "      img = cv2.imread(image_path)\n",
    "      img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "      img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
    "      # diving by 255\n",
    "      img_res /= 255.0\n",
    "\n",
    "      # annotation file\n",
    "      annot_filename = img_name[:-4] + '.xml'\n",
    "      annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
    "\n",
    "      boxes = []\n",
    "      labels = []\n",
    "      tree = et.parse(annot_file_path)\n",
    "      root = tree.getroot()\n",
    "\n",
    "      # cv2 image gives size as height x width\n",
    "      wt = img.shape[1]\n",
    "      ht = img.shape[0]\n",
    "\n",
    "      # box coordinates for xml files are extracted and corrected for image size given\n",
    "      for member in root.findall('object'):\n",
    "          labels.append(self.classes.index(member.find('name').text))\n",
    "\n",
    "          # bounding box\n",
    "          xmin = int(member.find('bndbox').find('xmin').text)\n",
    "          xmax = int(member.find('bndbox').find('xmax').text)\n",
    "          ymin = int(member.find('bndbox').find('ymin').text)\n",
    "          ymax = int(member.find('bndbox').find('ymax').text)\n",
    "\n",
    "          # Normalize coordinates to [0, 1] range\n",
    "          xmin_norm = max(0, min(xmin / wt, 1.0))\n",
    "          xmax_norm = max(0, min(xmax / wt, 1.0))\n",
    "          ymin_norm = max(0, min(ymin / ht, 1.0))\n",
    "          ymax_norm = max(0, min(ymax / ht, 1.0))\n",
    "\n",
    "          boxes.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "\n",
    "      # convert boxes into a torch.Tensor\n",
    "      boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "      # getting the areas of the boxes\n",
    "      area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "      # suppose all instances are not crowd\n",
    "      iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "      labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "      target = {}\n",
    "      target[\"boxes\"] = boxes\n",
    "      target[\"labels\"] = labels\n",
    "      target[\"area\"] = area\n",
    "      target[\"iscrowd\"] = iscrowd\n",
    "      # image_id\n",
    "      image_id = torch.tensor([idx])\n",
    "      target[\"image_id\"] = idx\n",
    "\n",
    "      if self.transforms:\n",
    "          # Convert labels to numpy array before passing to transforms\n",
    "          labels_np = labels.numpy()\n",
    "\n",
    "          sample = self.transforms(image=img_res,\n",
    "                                  bboxes=target['boxes'].numpy(),\n",
    "                                  labels=labels_np)\n",
    "\n",
    "          img_res = sample['image']\n",
    "          target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "          target['labels'] = torch.Tensor(sample['labels']).long()\n",
    "\n",
    "      return img_res, target\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "\n",
    "# check dataset\n",
    "dataset = SpeedBreakerDataset(files_dir, 224, 224)\n",
    "print('length of dataset = ', len(dataset), '\\n')\n",
    "\n",
    "# getting the image and target for a test index.  Feel free to change the index.\n",
    "img, target = dataset[78]\n",
    "print(img.shape, '\\n',target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for img, target in dataset:\n",
    "    print(\"Labels: \", target[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for img, target in dataset:\n",
    "    print(\"Labels: \", target[\"labels\"])\n",
    "    print(\"Bounding Boxes: \", target[\"boxes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_img_bbox(img, target):\n",
    "    # plot the image and bboxes\n",
    "    # Bounding boxes are defined as follows: x-min y-min width height\n",
    "    fig, a = plt.subplots(1,1)\n",
    "    fig.set_size_inches(5,5)\n",
    "    a.imshow(img)\n",
    "    for box in (target['boxes']):\n",
    "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "        rect = patches.Rectangle((x, y),\n",
    "                                 width, height,\n",
    "                                 linewidth = 2,\n",
    "                                 edgecolor = 'r',\n",
    "                                 facecolor = 'none')\n",
    "\n",
    "        # Draw the bounding box on top of the image\n",
    "        a.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# plotting the image with bboxes. Feel free to change the index\n",
    "img, target = dataset[56]\n",
    "plot_img_bbox(img, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_object_detection_model(num_classes):\n",
    "\n",
    "    # load a model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "                            A.HorizontalFlip(0.5),\n",
    "                            ToTensorV2(p=1.0)\n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                            ToTensorV2(p=1.0)\n",
    "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = SpeedBreakerDataset(files_dir, 224, 224, transforms= get_transform(train=True))\n",
    "dataset_test = SpeedBreakerDataset(files_dir, 224, 224, transforms= get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "# train test split\n",
    "test_split = 0.2\n",
    "tsize = int(len(dataset)*test_split)\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=10, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_object_detection_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "img, target = dataset[0]\n",
    "print(f\"Transformed image shape: {img.shape}\")\n",
    "print(f\"Transformed labels: {target['labels']}\")\n",
    "print(f\"Transformed bounding boxes: {target['boxes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, '/content/final_model.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
